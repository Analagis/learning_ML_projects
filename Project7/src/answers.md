`Propose a new method to initialize cluster centroids for K-means. List the advantages and disadvantages of Forgy, Random Partition, and your own initializations.`

Инициализация с помощью "разноудалённых центров" — выбрать первую точку случайно, а каждую следующую центроид искать как точку, максимально удалённую от уже выбранных (или от ближайшего ранее выбранного центра).
Можно использовать алгоритм k-means++: первая точка — случайно, остальные — с вероятностью, пропорциональной расстоянию до ближайшего уже выбранного центра.

Преимущества:
* Высокая вероятность равномерного распределения центров по данным.
* Уменьшается риск плохой инициализации и локального минимума.
* Часто ускоряет сходимость и повышает качество кластеризации.

Недостатки:
* Более сложная логика по сравнению с простым случайным выбором.
* Требует дополнительных вычислений по расстояниям между точками.

**Forgy**

Суть: выбирает k центров случайно из данных.

Преимущества:
* Простота реализации.
* Быстро работает даже на больших данных.

Недостатки:
* При неудачном выборе возможна плохая кластеризация (центры могут быть близко друг к другу).
* Может привести к медленной сходимости или некачественным результатам.

**Random Partition**

Суть: каждой точке случайно назначается кластер, затем находятся центры этих случайных групп.

Преимущества:
* Простая реализация.

Недостатки:
* Нередко на старте выбрана крайне неудачная разбивка, центры далеко от реальных кластеров.
* Очень низкая стабильность результата при многократных запусках.

`Describe how you can use clustering to speed up the KNN model. Hint: what is kd-tree?`

Можно разделить обучающую выборку на кластеры (например, с помощью k-means), а затем при поиске ближайших соседей для нового объекта сначала определить, к какому кластеру он ближе, и искать соседей только внутри этого подмножества. Это уменьшает число сверяемых точек.

kd-tree — это структура данных для быстрых поисков ближайших соседей в многомерном пространстве.
Строится дерево, где каждая вершина — деление пространства по одной координате; при поиске ближайшего соседа не нужно проверять все точки — дерево позволяет быстро исключать заведомо "далёкие" кластеры, ускоряя расчёт KNN.