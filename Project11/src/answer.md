## Encoder-Decoder architecture VS Attention

Недостаток: Вся информация о входной последовательности должна сжаться в одно фиксированное скрытое состояние Encoder'а, что создает бутылочное горлышко (bottleneck).

Последствия:
- Длинные последовательности — начало теряется к концу
- Фиксированная емкость — нельзя обработать произвольную длину
- Потеря позиционной информации — Decoder не видит порядок слов

Attention: Decoder смотрит на все скрытые состояния Encoder'а, а не только на последнее.

## Почему attention не различает порядок слов и как это исправить с помощью позиционного кодирования?

Attention работает как "мешок слов", потому что без дополнительной информации о позициях все токены на разных местах получают одинаковые векторы эмбеддингов. Чтобы модель понимала порядок, к каждому токену добавляется позиционное кодирование — специальный вектор, который кодирует его местоположение в последовательности.

Периодическое кодирование (sine/cosine) создаёт такие векторы с помощью синусов и косинусов разной частоты для каждой размерности. Главное преимущество: близкие позиции получают похожие векторы, которые attention легко отличает через скалярное произведение, а далёкие позиции — сильно разные.

Другой вариант - обучение в модели оптимальных векторов позиций как обычных параметров сети. Это гибкий подход: нейросеть во время тренировки находит такие позиционные эмбеддинги, которые наилучшим образом помогают attention различать порядок слов для конкретной задачи. В отличие от синусоид, нет фиксированной структуры — веса просто оптимизируются градиентным спуском вместе со всеми остальными параметрами модели.