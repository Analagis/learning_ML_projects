Project Summary
This project implements a sequence-to-sequence name translation pipeline from English to Russian using RNN-based models, evolving from vanilla encoder-decoder to attention mechanisms and positional encodings, with comprehensive evaluation of model performance and translation quality.

üìå Contents

1. Data Preparation

- –°–∫–∞—á–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å –∏–º–µ–Ω–∞–º–∏ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ train/valid/test (80-10-10%) —Å random_state.
- –°–æ–∑–¥–∞–Ω—ã —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö/—Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.

2. RNN Encoder
- –û–±—É—á–µ–Ω–∞ 1-—Å–ª–æ–π–Ω–∞—è RNN (GRU/LSTM) –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –∏–º–µ–Ω —Å early stopping –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ.
- –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ 10 –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä—É—Å—Å–∫–∏—Ö –∏–º–µ–Ω —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–Ω–∫–æ–¥–µ—Ä–∞.

3. RNN Machine Translation
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π RNN Encoder + —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω RNN Decoder (built-in –∫–ª–∞—Å—Å) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–∏—Ö –∏–º–µ–Ω.
- –û–±—É—á–µ–Ω —Ç–æ–ª—å–∫–æ Decoder (Encoder –∑–∞–º–æ—Ä–æ–∂–µ–Ω) —Å loss –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–π –±—É–∫–≤—ã –∏ early stopping.
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–≤–æ–¥–∞ (argmax –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –±—É–∫–≤—ã).
- Evaluation: –≥—Ä–∞—Ñ–∏–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ loss (train/valid), –ø–µ—Ä–µ–≤–æ–¥ 5 train + 5 valid –∏–º–µ–Ω –∫–∞–∂–¥—ã–µ n —ç–ø–æ—Ö, perplexity –Ω–∞ test.

4. Attention Machine Translation
- RNN Encoder –∏–∑ –ø—É–Ω–∫—Ç–∞ 3 + —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Attention Decoder (–±–µ–∑ built-in –∫–ª–∞—Å—Å–æ–≤).
- Attention –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–∞–º–∏ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –≤—Ö–æ–¥–∞–º–∏ –¥–µ–∫–æ–¥–µ—Ä–∞; –æ–±—É—á–µ–Ω —Ç–æ–ª—å–∫–æ Decoder.
- –ü–æ–≤—Ç–æ—Ä–µ–Ω–∞ –æ—Ü–µ–Ω–∫–∞ (4.e): loss curves, –ø–µ—Ä–µ–≤–æ–¥—ã –∏–º–µ–Ω, perplexity –Ω–∞ test.
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ vanilla RNN vs Attention –ø–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –∏ –º–µ—Ç—Ä–∏–∫–∞–º.

5. Positional Encoding Comparison
- Sine/Cosine PE: –æ–±—É—á–µ–Ω—ã Encoder, RNN Decoder –∏ Attention Decoder —Å —Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω—ã–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
- Trainable PE: –ø–æ–≤—Ç–æ—Ä–µ–Ω—ã —Ç–µ –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å –æ–±—É—á–∞–µ–º—ã–º–∏ –≤–µ—Å–∞–º–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è + –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è learned embeddings.
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö 6 –º–æ–¥–µ–ª–µ–π –ø–æ perplexity, –∫–∞—á–µ—Å—Ç–≤—É –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏.

6. Multi-head Attention
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω Multi-head Attention (n_heads=3) –¥–ª—è Machine Translation.
- –î–æ–±–∞–≤–ª–µ–Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention matrix –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∏–º–µ–Ω–∏ –∫–∞–∂–¥—ã–µ n —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è.
- –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ (loss curves, –ø–µ—Ä–µ–≤–æ–¥—ã, perplexity).
- –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã attention heads (–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ–≥–æ head —Å–æ —Å–¥–≤–∏–≥–æ–º 1).