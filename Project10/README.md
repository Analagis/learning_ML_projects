Project Summary
This project implements and compares recurrent neural networks (RNN, GRU, LSTM) for dual tasks of name generation and gender classification on a names dataset, analyzing vanishing gradients, multitask learning, perplexity metrics, and embedding visualizations.

üìå Contents

1. Introduction & RNN Fundamentals
- –û–ø–∏—Å–∞–Ω—ã —Ç–∏–ø—ã –∑–∞–¥–∞—á –¥–ª—è RNN (sequence modeling: –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏).
- –ü—Ä–∏–≤–µ–¥–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä: one-to-many (image captioning), many-to-many (–º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥), many-to-one (sentiment analysis).
- –ù–∞—Ä–∏—Å–æ–≤–∞–Ω–∞ —Å—Ö–µ–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–≤–∞ "Hello" —è–∑—ã–∫–æ–≤–æ–π RNN —Å –±–ª–æ–∫–∞–º–∏ embedding ‚Üí RNN ‚Üí linear + –ø–∞—Ä–∞–º–µ—Ç—Ä—ã hidden state (h‚ÇÄ = zeros).
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ vanilla RNN vs LSTM/GRU (gates –¥–ª—è –±–æ—Ä—å–±—ã —Å vanishing gradient) + —Å—Ö–µ–º–∞ LSTM/GRU.

2. Data Analysis & Preparation
- –ü–æ—Å—Ç—Ä–æ–µ–Ω—ã –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –¥–ª–∏–Ω –∏–º–µ–Ω, –ø—Ä–æ–≤–µ—Ä–µ–Ω –±–∞–ª–∞–Ω—Å –ø–æ –≥–µ–Ω–¥–µ—Ä—É, –ø–æ—Å—á–∏—Ç–∞–Ω–∞ —á–∞—Å—Ç–æ—Ç–∞ –±—É–∫–≤.
- –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–∞, –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ lowercase, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ <SOS>/<EOS>, padding –¥–æ max_length.
- –°–æ–∑–¥–∞–Ω —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä (–±—É–∫–≤—ã + —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã) —Å bidirectional mapping, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤.
- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/valid/test (80-10-10%) —Å random_state.

3. Vanilla RNN Implementation
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —Å–µ—Ç—å —Å –Ω—É–ª—è: Embedding ‚Üí custom RNN ‚Üí Linear(next token) + Linear(gender) + Sigmoid.
- –û–±—É—á–µ–Ω–∏–µ –Ω–∞ next-letter prediction (cross-entropy, teacher forcing), –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∞—è (softmax —Å temperature).
- Evaluation: –≥—Ä–∞—Ñ–∏–∫–∏ loss, 10 —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–º–µ–Ω, ROC AUC –Ω–∞ test –¥–ª—è gender classification.

4. GRU & LSTM Comparison
- –ü–æ–≤—Ç–æ—Ä–µ–Ω—ã —à–∞–≥–∏ 4 –¥–ª—è GRU –∏ LSTM (–∑–∞–º–µ–Ω–∞ recurrent block).
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ loss, –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, ROC AUC –ø–æ –≥–µ–Ω–¥–µ—Ä—É –¥–ª—è –≤—Å–µ—Ö 3 –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä.

5. Perplexity Metric Analysis
- –í—ã–≤–æ–¥ —Å–≤—è–∑–∏ perplexity ‚Üî cross-entropy, 2 —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ (—Å/–±–µ–∑ CE loss).
- –†–∞—Å—Å—á–∏—Ç–∞–Ω—ã baseline –∑–Ω–∞—á–µ–Ω–∏—è: random model –∏ most-popular-letters model.

6. Gender Classification Training
- –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ RNN/GRU/LSTM —Ç–æ–ª—å–∫–æ –Ω–∞ gender loss (BCE vs NLL), –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–π loss.
- Evaluation: loss curves, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–º–µ–Ω–∞, ROC AUC –Ω–∞ test.

7. Vanishing Gradient Analysis
- –ü–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —Å fixed-length –∏–º–µ–Ω–∞–º–∏ (mode), —Å–±–æ—Ä –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ embedding layer (register_backward_hook).
- –û—Ç–ª–∞–¥–∫–∞ –Ω–∞ batch_size=1, –∞–Ω–∞–ª–∏–∑ Frobenius norm –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –ø–æ–∑–∏—Ü–∏—è–º —Ç–æ–∫–µ–Ω–æ–≤.
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π mean gradient norm (RNN vs GRU vs LSTM) –¥–ª—è next-letter –∏ gender –∑–∞–¥–∞—á.

8. Multitask Learning
- –û–±—É—á–µ–Ω–∏–µ GRU/LSTM —Å joint loss (next-letter + gender classification).
- Evaluation: dual loss curves, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–º–µ–Ω–∞, ROC AUC –ø–æ –≥–µ–Ω–¥–µ—Ä—É.

9. Embeddings Visualization
- t-SNE –∏ PCA —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ embedding'–æ–≤ –±—É–∫–≤ –∏–∑ multitask –º–æ–¥–µ–ª–∏.
- Scatterplot –≥–ª–∞—Å–Ω—ã—Ö vs —Å–æ–≥–ª–∞—Å–Ω—ã—Ö, –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤ 2D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.